{"name": "Create List", "icon": null, "is_component": true, "endpoint_name": null, "data": {"edges": [], "nodes": [{"data": {"type": "CreateListFromFileWithURLParsingAndMetadataExtraction", "node": {"template": {"_type": "Component", "file_path": {"trace_as_metadata": true, "file_path": "c4046d27-af41-4945-8170-238e75e596bf\\2024-10-04_15-32-32_Saved_Items.csv", "fileTypes": ["csv", "json", "txt"], "list": false, "required": true, "placeholder": "", "show": true, "name": "file_path", "value": "Saved_Items.csv", "display_name": "File Path", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "file", "_input_type": "FileInput", "load_from_db": false}, "api_key": {"load_from_db": false, "required": true, "placeholder": "", "show": true, "name": "api_key", "value": null, "display_name": "OpenAI API Key", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The OpenAI API Key to use for the OpenAI model.", "title_case": false, "password": true, "type": "str", "_input_type": "SecretStrInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "import re\r\nimport pandas as pd\r\nimport json\r\nimport os\r\nimport asyncio\r\nfrom langchain_community.document_loaders.web_base import WebBaseLoader\r\nfrom langflow.custom import Component\r\nfrom langflow.inputs import StrInput, FileInput, SecretStrInput\r\nfrom langflow.schema import Data\r\nfrom langflow.template import Output\r\nfrom datetime import datetime\r\nimport logging\r\n\r\nfrom langchain_openai import ChatOpenAI\r\nfrom langchain.schema import SystemMessage, HumanMessage  # Imported message classes\r\n\r\n# Set up logging\r\nlogging.basicConfig(\r\n    level=logging.INFO,\r\n    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',\r\n    handlers=[\r\n        logging.FileHandler(\"component.log\"),\r\n        logging.StreamHandler()\r\n    ]\r\n)\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass CreateListComponent(Component):\r\n    display_name = \"Create List from File with URL Parsing and Metadata Extraction\"\r\n    description = \"Creates a list of texts from a file, fetches content from URLs if present, and extracts metadata using OpenAI's LLM.\"\r\n    icon = \"list\"\r\n    name = \"CreateListFromFileWithURLParsingAndMetadataExtraction\"\r\n\r\n    inputs = [\r\n        FileInput(\r\n            name=\"file_path\",\r\n            display_name=\"File Path\",\r\n            file_types=[\"csv\", \"json\", \"txt\"],  # Allow different file types\r\n            required=True,\r\n        ),\r\n        StrInput(\r\n            name=\"column_name\",\r\n            display_name=\"Column Name (for CSV/JSON)\",\r\n            info=\"Specify the column name if using CSV or JSON files.\",\r\n            required=False,\r\n        ),\r\n        SecretStrInput(  # Input for OpenAI API Key\r\n            name=\"api_key\",\r\n            display_name=\"OpenAI API Key\",\r\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\r\n            required=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Data List\", name=\"list\", method=\"create_list\"),\r\n    ]\r\n\r\n    def ensure_url(self, string: str) -> str:\r\n        \"\"\"Ensures the given string is a valid URL.\"\"\"\r\n        if not string.startswith((\"http://\", \"https://\")):\r\n            string = \"http://\" + string\r\n\r\n        # Basic URL validation regex\r\n        url_regex = re.compile(\r\n            r\"^(https?:\\/\\/)\"\r\n            r\"(www\\.)?\"\r\n            r\"([a-zA-Z0-9.-]+)\"\r\n            r\"(\\.[a-zA-Z]{2,})\"\r\n            r\"(:\\d+)?\"\r\n            r\"(\\/[^\\s]*)?$\",\r\n            re.IGNORECASE,\r\n        )\r\n\r\n        if not url_regex.match(string):\r\n            raise ValueError(f\"Invalid URL: {string}\")\r\n\r\n        return string\r\n\r\n    async def generate_prompt(self, content: str) -> str:\r\n        \"\"\"Constructs a prompt for the LLM to extract metadata from URL content.\"\"\"\r\n        prompt = f\"\"\"\r\n        You are a helpful assistant. Given the following URL and webpage content, extract metadata.\r\n\r\n\r\n        Content: {content}\r\n\r\n        Please return only the data in the following structured JSON format without any additional text:\r\n\r\n        {{\r\n            \"name\": \"The name of the webpage or article\",\r\n            \"number_of_likes\": 0,  # The number of likes (if available)\r\n            \"comments\": [],  # List of comments (if available)\r\n            \"author\": \"Author of the content (if available)\",\r\n            \"publication_date\": \"YYYY-MM-DD\"  # Publication date in YYYY-MM-DD format (if available)\r\n        }}\r\n\r\n        If any information is missing, set the field to an empty string or an empty list, as appropriate.\r\n\r\n        Example:\r\n        {{\r\n            \"name\": \"Example Article\",\r\n            \"number_of_likes\": 100,\r\n            \"comments\": [\"Great article!\", \"Very informative.\"],\r\n            \"author\": \"Jane Doe\",\r\n            \"publication_date\": \"2023-08-15\"\r\n        }}\r\n        \"\"\"\r\n        return prompt\r\n\r\n    async def generate_metadata_from_url_and_content(self, url: str, content: str) -> dict:\r\n        \"\"\"Uses an LLM to extract metadata by passing both the URL and content.\"\"\"\r\n        prompt = await self.generate_prompt(content)\r\n        metadata_str = \"\"  # Initialize to ensure it's always defined\r\n\r\n        try:\r\n            # Initialize ChatOpenAI with parameters\r\n            chat_openai = ChatOpenAI(\r\n                model=\"gpt-4\",\r\n                temperature=0.1,  # Default temperature; adjust as needed\r\n                max_tokens=500,\r\n                api_key=self.api_key,  # Use the API key directly as a string\r\n            )\r\n\r\n            # Construct messages using proper message classes\r\n            messages = [\r\n                SystemMessage(content=\"You are a helpful assistant.\"),\r\n                HumanMessage(content=prompt)\r\n            ]\r\n\r\n            # Make an asynchronous call to the LLM with a list of message lists\r\n            response = await chat_openai.agenerate([messages])\r\n\r\n            # Access the generated text\r\n            metadata_str = response.generations[0][0].text.strip()\r\n            \r\n            # Extract JSON using regex to find the first JSON object in the response\r\n            json_match = re.search(r'\\{.*\\}', metadata_str, re.DOTALL)\r\n            if json_match:\r\n                metadata_str = json_match.group()\r\n            else:\r\n                raise ValueError(\"No JSON object found in the LLM response.\")\r\n\r\n            metadata = json.loads(metadata_str)\r\n\r\n            return {\r\n              \r\n                \"name\": metadata.get(\"name\", \"Unknown\"),\r\n                \"number_of_likes\": int(metadata.get(\"number_of_likes\", 0)) if isinstance(metadata.get(\"number_of_likes\", 0), (int, float, str)) else 0,\r\n                \"comments\": metadata.get(\"comments\", []),\r\n                \"author\": metadata.get(\"author\", \"Unknown\"),\r\n                \"publication_date\": metadata.get(\"publication_date\", \"\")\r\n            }\r\n\r\n        except json.JSONDecodeError as jde:\r\n            logger.error(f\"JSON decoding failed for URL {url}: {jde}\")\r\n            logger.debug(f\"LLM Response: {metadata_str}\")\r\n            return {\r\n              \r\n                \"name\": \"Unknown\",\r\n                \"number_of_likes\": 0,\r\n                \"comments\": [],\r\n                \"author\": \"Unknown\",\r\n                \"publication_date\": \"\"\r\n            }\r\n        except Exception as e:\r\n            logger.error(f\"An error occurred while generating metadata for URL {url}: {e}\")\r\n            return {\r\n                \r\n                \"name\": \"Unknown\",\r\n                \"number_of_likes\": 0,\r\n                \"comments\": [],\r\n                \"author\": \"Unknown\",\r\n                \"publication_date\": \"\"\r\n            }\r\n\r\n    async def fetch_url_content(self, urls: list[str]) -> list[Data]:\r\n        \"\"\"Fetches content from the provided URLs and uses LLM for metadata extraction.\"\"\"\r\n        urls = [self.ensure_url(url.strip()) for url in urls if url.strip()]\r\n        loader = WebBaseLoader(web_paths=urls, encoding=\"utf-8\")\r\n        docs = loader.load()\r\n        data_list = []\r\n\r\n        logger.info(f\"Fetched {len(docs)} documents from URLs.\")\r\n        url_docs=zip(docs,urls)\r\n        # Loop through each document\r\n        for doc,link in url_docs:\r\n            url = link\r\n            content = doc.page_content\r\n            logger.info(f\"Processing URL: {url}\")\r\n\r\n            metadata = await self.generate_metadata_from_url_and_content(url, content)\r\n            \r\n            # Create a Data object with the metadata returned from the LLM\r\n            data = Data(\r\n                text=content,\r\n                url=url,\r\n                timestamp=datetime.utcnow().isoformat(),\r\n                name=metadata.get(\"name\"),\r\n                number_of_likes=str(metadata.get(\"number_of_likes\", 0)),\r\n                comments=metadata.get(\"comments\", []),\r\n                author=metadata.get(\"author\", \"Unknown\"),\r\n                publication_date=metadata.get(\"publication_date\", \"\")\r\n            )\r\n            data_list.append(data)\r\n\r\n        return data_list\r\n\r\n    async def create_list(self) -> list[Data]:\r\n        data_list = []\r\n        urls = []\r\n        \r\n        # Determine file type and parse accordingly\r\n        if self.file_path.endswith(\".csv\"):\r\n            logger.info(f\"Processing CSV file: {self.file_path}\")\r\n            df = pd.read_csv(self.file_path)\r\n            if self.column_name and self.column_name in df.columns:\r\n                texts = df[self.column_name].tolist()\r\n                logger.info(f\"Extracted texts from column '{self.column_name}'.\")\r\n            else:\r\n                texts = df.to_string(index=False).splitlines()\r\n                logger.info(f\"No column specified or column not found. Extracted all lines as texts.\")\r\n        elif self.file_path.endswith(\".json\"):\r\n            logger.info(f\"Processing JSON file: {self.file_path}\")\r\n            with open(self.file_path, 'r', encoding='utf-8') as f:\r\n                json_data = json.load(f)\r\n                if self.column_name and self.column_name in json_data:\r\n                    texts = json_data[self.column_name]\r\n                    logger.info(f\"Extracted texts from key '{self.column_name}'.\")\r\n                else:\r\n                    texts = [json.dumps(json_data)]\r\n                    logger.info(f\"No key specified or key not found. Serialized entire JSON object as a single text entry.\")\r\n        else:\r\n            logger.info(f\"Processing TXT file: {self.file_path}\")\r\n            with open(self.file_path, 'r', encoding='utf-8') as f:\r\n                texts = f.readlines()\r\n            logger.info(f\"Extracted {len(texts)} lines from TXT file.\")\r\n\r\n        # Separate URLs from regular text\r\n        for text in texts:\r\n            text = text.strip()\r\n            if re.match(r'^(https?:\\/\\/)', text):\r\n                urls.append(text)\r\n                logger.info(f\"Identified URL: {text}\")\r\n            else:\r\n                logger.info(f\"Non-URL text found and skipped: {text}\")\r\n\r\n        # If URLs are present, fetch their content and extract metadata\r\n        if urls:\r\n            fetched_data = await self.fetch_url_content(urls)\r\n            data_list.extend(fetched_data)\r\n            logger.info(f\"Added {len(fetched_data)} Data objects from URLs.\")\r\n        else:\r\n            logger.warning(\"No URLs found in the input file.\")\r\n\r\n        self.status = data_list\r\n        logger.info(\"Data list creation completed.\")\r\n        return data_list\r\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "column_name": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "column_name", "value": "savedItem", "display_name": "Column Name (for CSV/JSON)", "advanced": false, "dynamic": false, "info": "Specify the column name if using CSV or JSON files.", "title_case": false, "type": "str", "_input_type": "StrInput"}}, "description": "Creates a list of texts from a file, fetches content from URLs if present, and extracts metadata using OpenAI's LLM.", "icon": "list", "base_classes": ["Data"], "display_name": "Create List", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Data"], "selected": "Data", "name": "list", "display_name": "Data List", "method": "create_list", "value": "__UNDEFINED__", "cache": true}], "field_order": ["file_path", "column_name", "api_key"], "beta": false, "edited": true, "lf_version": "1.0.17", "official": false}, "id": "CreateListFromFileWithURLParsingAndMetadataExtraction-6FFO9"}, "id": "CreateListFromFileWithURLParsingAndMetadataExtraction-6FFO9", "position": {"x": 0, "y": 0}, "type": "genericNode"}], "viewport": {"x": 1, "y": 1, "zoom": 1}}, "user_id": "36db9754-af63-4a86-aa22-d532aa59e9f2", "folder_id": "f28a1646-38b2-4ca3-aa4b-e2dd43d69d9a", "description": "Creates a list of texts from a file, fetches content from URLs if present, and extracts metadata using OpenAI's LLM.", "icon_bg_color": null, "updated_at": "2024-10-04T10:15:59+00:00", "webhook": false, "id": "daeff8d8-25de-493d-b91f-32dcc5b2c1cb"}